{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor, ResultsWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"ALE/MsPacman-ram-v5\", render_mode='rgb_array')  \n",
    "train_monitor = Monitor(env, filename=\"monitors/ppo_train\")\n",
    "test_monitor = Monitor(env, filename=\"monitors/ppo_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample action: 8\n",
      "observation space shape: (128,)\n",
      "sample observation: [159 140   0 101 241 163 201  87  48 238  73 179  81  66 150 126  92   4\n",
      " 119  73  82 237  62 104  11  69 115 118  53 236 203 199 232  53 103 150\n",
      "   4 248 146 245  89 216  86  45 243  59 169  63   2 214  89 150 138 248\n",
      "   0   4  15  53 234  77 240 201 167 135  72 203 203  76 172 239  32 221\n",
      "  75  38  36 157 241  54 230   5 246 231 191 219  35   6 139 118 108 255\n",
      " 234 102 206 181 242 148  32 129 172 167 248 187 157 248  78 127 111  89\n",
      " 123 116 123 225 229  29  51  63  84 205 133 187 151 249 175 177 206  44\n",
      " 251  12]\n"
     ]
    }
   ],
   "source": [
    "train_monitor.reset()\n",
    "\n",
    "# sample action:\n",
    "print(\"sample action:\", train_monitor.action_space.sample())\n",
    "\n",
    "# observation space shape:\n",
    "print(\"observation space shape:\", train_monitor.observation_space.shape)\n",
    "\n",
    "# sample observation:\n",
    "print(\"sample observation:\", train_monitor.observation_space.sample())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100, episode_reward=520.00 +/- 160.00\n",
      "Episode length: 686.20 +/- 38.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200, episode_reward=532.00 +/- 155.10\n",
      "Episode length: 658.60 +/- 65.68\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300, episode_reward=534.00 +/- 178.06\n",
      "Episode length: 654.20 +/- 73.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400, episode_reward=622.00 +/- 218.85\n",
      "Episode length: 724.60 +/- 96.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=500, episode_reward=536.00 +/- 152.92\n",
      "Episode length: 720.20 +/- 85.46\n",
      "Eval num_timesteps=600, episode_reward=450.00 +/- 20.00\n",
      "Episode length: 669.40 +/- 32.06\n",
      "Eval num_timesteps=700, episode_reward=440.00 +/- 0.00\n",
      "Episode length: 663.40 +/- 26.30\n",
      "Eval num_timesteps=800, episode_reward=600.00 +/- 195.96\n",
      "Episode length: 706.20 +/- 48.03\n",
      "Eval num_timesteps=900, episode_reward=444.00 +/- 4.90\n",
      "Episode length: 678.60 +/- 31.68\n",
      "Eval num_timesteps=1000, episode_reward=440.00 +/- 0.00\n",
      "Episode length: 655.00 +/- 19.39\n",
      "Eval num_timesteps=1100, episode_reward=530.00 +/- 180.00\n",
      "Episode length: 661.40 +/- 50.44\n",
      "Eval num_timesteps=1200, episode_reward=524.00 +/- 158.19\n",
      "Episode length: 693.00 +/- 43.16\n",
      "Eval num_timesteps=1300, episode_reward=520.00 +/- 160.00\n",
      "Episode length: 669.40 +/- 67.71\n",
      "Eval num_timesteps=1400, episode_reward=522.00 +/- 159.05\n",
      "Episode length: 638.20 +/- 66.42\n",
      "Eval num_timesteps=1500, episode_reward=442.00 +/- 4.00\n",
      "Episode length: 657.40 +/- 44.39\n",
      "Eval num_timesteps=1600, episode_reward=448.00 +/- 7.48\n",
      "Episode length: 656.60 +/- 58.22\n",
      "Eval num_timesteps=1700, episode_reward=612.00 +/- 207.21\n",
      "Episode length: 695.40 +/- 64.91\n",
      "Eval num_timesteps=1800, episode_reward=520.00 +/- 160.00\n",
      "Episode length: 689.80 +/- 48.89\n",
      "Eval num_timesteps=1900, episode_reward=520.00 +/- 160.00\n",
      "Episode length: 657.40 +/- 57.50\n",
      "Eval num_timesteps=2000, episode_reward=454.00 +/- 28.00\n",
      "Episode length: 634.20 +/- 49.60\n",
      "Eval num_timesteps=2100, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=2200, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=2300, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=2400, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=2600, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=2800, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=2900, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3100, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3200, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3300, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3400, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3700, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3800, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=3900, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=120.00 +/- 0.00\n",
      "Episode length: 425.00 +/- 0.00\n",
      "Eval num_timesteps=4100, episode_reward=240.00 +/- 48.99\n",
      "Episode length: 443.80 +/- 30.37\n",
      "Eval num_timesteps=4200, episode_reward=256.00 +/- 38.26\n",
      "Episode length: 431.40 +/- 24.80\n",
      "Eval num_timesteps=4300, episode_reward=278.00 +/- 4.00\n",
      "Episode length: 419.00 +/- 0.00\n",
      "Eval num_timesteps=4400, episode_reward=258.00 +/- 39.19\n",
      "Episode length: 431.40 +/- 24.80\n",
      "Eval num_timesteps=4500, episode_reward=218.00 +/- 46.65\n",
      "Episode length: 450.60 +/- 27.75\n",
      "Eval num_timesteps=4600, episode_reward=260.00 +/- 40.00\n",
      "Episode length: 431.40 +/- 24.80\n",
      "Eval num_timesteps=4700, episode_reward=240.00 +/- 48.99\n",
      "Episode length: 443.80 +/- 30.37\n",
      "Eval num_timesteps=4800, episode_reward=278.00 +/- 4.00\n",
      "Episode length: 422.20 +/- 6.40\n",
      "Eval num_timesteps=4900, episode_reward=280.00 +/- 0.00\n",
      "Episode length: 419.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=216.00 +/- 48.41\n",
      "Episode length: 456.20 +/- 30.37\n",
      "Eval num_timesteps=5100, episode_reward=256.00 +/- 38.26\n",
      "Episode length: 431.40 +/- 24.80\n",
      "Eval num_timesteps=5200, episode_reward=218.00 +/- 46.65\n",
      "Episode length: 456.20 +/- 30.37\n",
      "Eval num_timesteps=5300, episode_reward=258.00 +/- 39.19\n",
      "Episode length: 431.40 +/- 24.80\n",
      "Eval num_timesteps=5400, episode_reward=258.00 +/- 39.19\n",
      "Episode length: 431.40 +/- 24.80\n",
      "Eval num_timesteps=5500, episode_reward=260.00 +/- 40.00\n",
      "Episode length: 431.80 +/- 24.61\n",
      "Eval num_timesteps=5600, episode_reward=278.00 +/- 4.00\n",
      "Episode length: 419.00 +/- 0.00\n",
      "Eval num_timesteps=5700, episode_reward=218.00 +/- 50.75\n",
      "Episode length: 456.20 +/- 30.37\n",
      "Eval num_timesteps=5800, episode_reward=238.00 +/- 51.54\n",
      "Episode length: 443.80 +/- 30.37\n",
      "Eval num_timesteps=5900, episode_reward=238.00 +/- 47.50\n",
      "Episode length: 443.80 +/- 30.37\n",
      "Eval num_timesteps=6000, episode_reward=256.00 +/- 38.26\n",
      "Episode length: 431.40 +/- 24.80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# logge:r https://stable-baselines3.readthedocs.io/en/master/common/logger.html\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:201\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[0;32m    200\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos, dones)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:460\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;66;03m# Reset success rate buffer\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_success_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 460\u001b[0m episode_rewards, episode_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_success_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(episode_rewards, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\policies.py:717\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\policies.py:750\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Distribution:\n\u001b[0;32m    744\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m    Get the current policy distribution given the observations.\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m    :param obs:\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    :return: the action distribution.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi_features_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m     latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(features)\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\policies.py:130\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs, features_extractor: BaseFeaturesExtractor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    Preprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    :return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     preprocessed_obs \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\ML2\\lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:121\u001b[0m, in \u001b[0;36mpreprocess_obs\u001b[1;34m(obs, observation_space, normalize_images)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_images \u001b[38;5;129;01mand\u001b[39;00m is_image_space(observation_space):\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# One hot encoding and convert to float to avoid errors\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mone_hot(obs\u001b[38;5;241m.\u001b[39mlong(), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(observation_space\u001b[38;5;241m.\u001b[39mn))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model creation and learning\n",
    "eval_callback = EvalCallback(train_monitor, best_model_save_path=\"./logs/logsPPO/\",\n",
    "                             log_path=\"./logs/logsPPO\", eval_freq=100,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "# logge:r https://stable-baselines3.readthedocs.io/en/master/common/logger.html\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=100000, log_interval=4, callback=eval_callback)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"logs/logsPPO/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the results with 50 episodes\n",
    "episodes = 3\n",
    "frames = []\n",
    "for ep in range(episodes):\n",
    "    obs = test_monitor.reset()\n",
    "    obs = obs[0]\n",
    "    done = False\n",
    "    rew = 0\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, tr, info = test_monitor.step(action)\n",
    "        img = test_monitor.render()\n",
    "        frames.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now show the animation:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rcParams['animation.embed_limit'] = 80  # For example, set it to 30 MB\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "def update_scene(num, frames, patch):\n",
    "    # print(num)\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_monitor.get_episode_rewards())\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Rewards per episode\")\n",
    "plt.title(\"Train Rewards\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(test_monitor.get_episode_rewards())\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Rewards per episode\")\n",
    "plt.title(\"Test Rewards\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
